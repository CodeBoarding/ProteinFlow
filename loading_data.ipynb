{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sidechainnet as scn\n",
    "import pickle\n",
    "from utils.preprocess_sidechainnet import combine_data, preprocess, cut_missing_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the 70% identity dataset and expand it with sequences from the same PDBs in the 100% identity dataset. \n",
    "\n",
    "This is by far the most memory-intensive step so we will save the resulting intermediate file. I have uploaded my output from this step to S3.\n",
    "\n",
    "To download it, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp s3://ml4-main-storage/updated_70.pkl sidechainnet_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SidechainNet was loaded from ./sidechainnet_data/sidechainnet_casp12_70.pkl.\n",
      "SidechainNet was loaded from ./sidechainnet_data/sidechainnet_casp12_100.pkl.\n",
      "Adding 11332 chains...\n",
      "Adding 0 chains...\n",
      "Adding 0 chains...\n",
      "Adding 0 chains...\n",
      "Adding 0 chains...\n",
      "Adding 0 chains...\n",
      "Adding 0 chains...\n",
      "Adding 0 chains...\n"
     ]
    }
   ],
   "source": [
    "# data_70 = scn.load(casp_version=12, thinning=70)\n",
    "# data_100 = scn.load(casp_version=12, thinning=100)\n",
    "# for dataset in [\"train\"]:\n",
    "#     data_70 = combine_data(\n",
    "#         main_data_scn=data_70, \n",
    "#         add_data_scn=data_100,\n",
    "#         dataset=dataset,\n",
    "#     )\n",
    "# with open(\"./sidechainnet_data/updated_casp12_70.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(data_70, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will open that file, filter the chains by name, resolution, length and fraction of missing values and combine them in a new multi-chain dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for shortened chains...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32446/32446 [00:00<00:00, 628871.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for duplicates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6684/6684 [00:00<00:00, 333941.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sequence similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4136/4136 [00:08<00:00, 515.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding chains to combine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3317/3317 [00:00<00:00, 1017665.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering by resolution, length and missing values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52712/52712 [00:01<00:00, 28451.69it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"./sidechainnet_data/updated_70.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train = preprocess(data, dataset=\"train\")\n",
    "validation = preprocess(data, dataset=\"valid-30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we can cut the ends where there is structure information missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutting missing ends...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24276/24276 [00:00<00:00, 192420.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut 18474 start intervals (mean length 10.8) and 19713 end intervals (mean length 15.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = cut_missing_ends(train)\n",
    "validation = cut_missing_ends(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final dataset. We will save it in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sidechainnet_data/multichain_casp12_70.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train, f)\n",
    "with open(\"./sidechainnet_data/multichain_casp12_70_val10.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is organised similarly to SidechainNet datasets but there are some important differences.\n",
    "\n",
    "It is a dictionary with the following keys:\n",
    "- `'ids'`: a list of PDB IDs,\n",
    "- `'scn'`: a lisf of lists of SidechainNet IDs that were combined here,\n",
    "- `'seq'`: a list of lists of residue sequences,\n",
    "- `'msk'`: a list of lists of string masks (e.g. `'+++-----+++++++++++++++'`; `'-'` indicates missing structural information),\n",
    "- `'crd'`: a list of `numpy` coordinate arrays of shape `(L * 14, 3)` (first 4 sets of coordinates correspond to N, Ca, C, O, in that order; the rest is side chain information)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
