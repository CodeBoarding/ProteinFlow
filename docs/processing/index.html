<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>proteinflow.processing API documentation</title>
<meta name="description" content="Functions for processing PDB files and generating new datasets." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="shortcut icon" type="image/x-icon" href="logo.png?">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>proteinflow.processing</code></h1>
</header>
<section id="section-intro">
<p>Functions for processing PDB files and generating new datasets.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Functions for processing PDB files and generating new datasets.&#34;&#34;&#34;
import multiprocessing
import os
import pickle
import subprocess
from collections import Counter
from datetime import datetime

import editdistance
import numpy as np
import requests
from joblib import Parallel, delayed
from p_tqdm import p_map
from tqdm import tqdm

from proteinflow.data import PDBEntry, SAbDabEntry
from proteinflow.data.utils import PDBError
from proteinflow.download import _load_files
from proteinflow.ligand import _compare_smiles
from proteinflow.logging import _log_exception, _log_removed, get_error_summary


def run_processing(
    tmp_folder=&#34;./data/tmp_pdb&#34;,
    output_folder=&#34;./data/pdb&#34;,
    min_length=30,
    max_length=10000,
    resolution_thr=3.5,
    missing_ends_thr=0.3,
    missing_middle_thr=0.1,
    filter_methods=True,
    remove_redundancies=False,
    redundancy_thr=0.9,
    n=None,
    force=False,
    tag=None,
    pdb_snapshot=None,
    load_live=False,
    sabdab=False,
    sabdab_data_path=None,
    require_antigen=False,
    max_chains=5,
    pdb_id_list_path=None,
    load_ligands=False,
    require_ligand=False,
):
    &#34;&#34;&#34;Download and parse PDB files that meet filtering criteria.

    The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are
    the following:

    - `&#39;crd_bb&#39;`: a `numpy` array of shape `(L, 4, 3)` with backbone atom coordinates (N, C, CA, O),
    - `&#39;crd_sc&#39;`: a `numpy` array of shape `(L, 10, 3)` with sidechain atom coordinates (in a fixed order, check `sidechain_order()`),
    - `&#39;msk&#39;`: a `numpy` array of shape `(L,)` where ones correspond to residues with known coordinates and
        zeros to missing values,
    - `&#39;seq&#39;`: a string of length `L` with residue types.

    When creating a SAbDab dataset, an additional key is added to the dictionary:
    - `&#39;cdr&#39;`: a `&#39;numpy&#39;` array of shape `(L,)` where CDR residues are marked with the corresponding type (`&#39;H1&#39;`, `&#39;L1&#39;`, ...)
        and non-CDR residues are marked with `&#39;-&#39;`.

    All errors including reasons for filtering a file out are logged in a log file.

    Parameters
    ----------
    tmp_folder : str, default &#34;./data/tmp_pdb&#34;
        The folder where temporary files will be saved
    output_folder : str, default &#34;./data/pdb&#34;
        The folder where the output files will be saved
    min_length : int, default 30
        The minimum number of non-missing residues per chain
    max_length : int, default 10000
        The maximum number of residues per chain (set None for no threshold)
    resolution_thr : float, default 3.5
        The maximum resolution
    missing_ends_thr : float, default 0.3
        The maximum fraction of missing residues at the ends
    missing_middle_thr : float, default 0.1
        The maximum fraction of missing residues in the middle (after missing ends are disregarded)
    filter_methods : bool, default True
        If `True`, only files obtained with X-ray or EM will be processed
    remove_redundancies : bool, default False
        If `True`, removes biounits that are doubles of others sequence wise
    redundancy_thr : float, default 0.9
        The threshold upon which sequences are considered as one and the same (default: 90%)
    n : int, default None
        The number of files to process (for debugging purposes)
    force : bool, default False
        When `True`, rewrite the files if they already exist
    tag : str, optional
        A tag to add to the log file
    pdb_snapshot : str, optional
        the PDB snapshot to use, by default the latest is used (if `sabdab` is `True`, you can use any date in the format YYYYMMDD as a cutoff)
    load_live : bool, default False
        if `True`, load the files that are not in the latest PDB snapshot from the PDB FTP server (forced to `False` if `pdb_snapshot` is not `None`)
    sabdab : bool, default False
        if `True`, download the SAbDab database instead of PDB
    sabdab_data_path : str, optional
        path to a zip file or a directory containing SAbDab files (only used if `sabdab` is `True`)
    require_antigen : bool, default False
        if `True`, only keep files with antigen chains (only used if `sabdab` is `True`)
    max_chains : int, default 5
        the maximum number of chains per biounit
    pdb_id_list_path : str, default None
        if provided, get pdb_ids from list (format pdb_id-num example: 1XYZ-1)
    load_ligands: boool, default False
        Whether or not to load the ligands in the pdbs
    require_ligand: bool, default False
        if `True`, only keep files with ligands

    Returns
    -------
    log : dict
        a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors

    &#34;&#34;&#34;
    TMP_FOLDER = tmp_folder
    OUTPUT_FOLDER = output_folder
    MIN_LENGTH = min_length
    MAX_LENGTH = max_length
    RESOLUTION_THR = resolution_thr
    MISSING_ENDS_THR = missing_ends_thr
    MISSING_MIDDLE_THR = missing_middle_thr

    if not os.path.exists(TMP_FOLDER):
        os.makedirs(TMP_FOLDER)
    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)

    LOG_FILE = os.path.join(OUTPUT_FOLDER, &#34;log.txt&#34;)
    print(f&#34;Log file: {LOG_FILE} \n&#34;)
    now = datetime.now()  # current date and time
    date_time = now.strftime(&#34;%m/%d/%Y, %H:%M:%S&#34;) + &#34;\n\n&#34;
    with open(LOG_FILE, &#34;a&#34;) as f:
        f.write(date_time)
        if tag is not None:
            f.write(f&#34;tag: {tag} \n&#34;)
        f.write(f&#34;    min_length: {min_length} \n&#34;)
        f.write(f&#34;    max_length: {max_length} \n&#34;)
        f.write(f&#34;    resolution_thr: {resolution_thr} \n&#34;)
        f.write(f&#34;    missing_ends_thr: {missing_ends_thr} \n&#34;)
        f.write(f&#34;    missing_middle_thr: {missing_middle_thr} \n&#34;)
        f.write(f&#34;    filter_methods: {filter_methods} \n&#34;)
        f.write(f&#34;    remove_redundancies: {remove_redundancies} \n&#34;)
        f.write(f&#34;    sabdab: {sabdab} \n&#34;)
        f.write(f&#34;    pdb_snapshot: {pdb_snapshot} \n&#34;)
        f.write(f&#34;    max_chains: {max_chains} \n&#34;)
        if remove_redundancies:
            f.write(f&#34;    redundancy_threshold: {redundancy_thr} \n&#34;)
        if sabdab:
            f.write(f&#34;    require_antigen: {require_antigen} \n&#34;)
            f.write(f&#34;    sabdab_data_path: {sabdab_data_path} \n&#34;)
        else:
            f.write(f&#34;    load_live: {load_live} \n&#34;)
        f.write(&#34;\n&#34;)

    def process_f(
        local_paths,
        show_error=False,
        force=True,
        sabdab=False,
        ligand=False,
    ):
        pdb_path, fasta_path = local_paths
        chain_id = None
        if sabdab:
            pdb_path, chain_id = pdb_path
            heavy, light, antigen = chain_id.split(&#34;_&#34;)
            if heavy == &#34;nan&#34;:
                heavy = None
            if light == &#34;nan&#34;:
                light = None
            if antigen == &#34;nan&#34;:
                antigen = []
            else:
                antigen = antigen.split(&#34; | &#34;)
        fn = os.path.basename(pdb_path)
        pdb_id = fn.split(&#34;.&#34;)[0]
        if os.path.getsize(pdb_path) &gt; 1e7:
            _log_exception(
                PDBError(&#34;PDB / mmCIF file is too large&#34;),
                LOG_FILE,
                pdb_id,
                TMP_FOLDER,
                chain_id=chain_id,
            )
        try:
            # local_path = download_f(pdb_id, s3_client=s3_client, load_live=load_live)
            name = pdb_id if not sabdab else pdb_id + &#34;-&#34; + chain_id
            target_file = os.path.join(OUTPUT_FOLDER, name + &#34;.pickle&#34;)
            if not force and os.path.exists(target_file):
                raise PDBError(&#34;File already exists&#34;)
            if sabdab:
                pdb_entry = SAbDabEntry(
                    pdb_path=pdb_path,
                    heavy_chain=heavy,
                    light_chain=light,
                    antigen_chains=antigen,
                    fasta_path=fasta_path,
                )
            else:
                pdb_entry = PDBEntry(
                    pdb_path=pdb_path, fasta_path=fasta_path, load_ligand=ligand
                )
            # filter and convert
            protein_dict = filter_and_convert(
                pdb_entry,
                min_length=MIN_LENGTH,
                max_length=MAX_LENGTH,
                max_missing_ends=MISSING_ENDS_THR,
                max_missing_middle=MISSING_MIDDLE_THR,
                load_ligands=ligand,
                require_ligand=require_ligand,
            )
            # save
            with open(target_file, &#34;wb&#34;) as f:
                pickle.dump(protein_dict, f)
        except Exception as e:
            if show_error:
                raise e
            else:
                _log_exception(e, LOG_FILE, pdb_id, TMP_FOLDER, chain_id=chain_id)

    try:
        paths, error_ids = _load_files(
            resolution_thr=RESOLUTION_THR,
            filter_methods=filter_methods,
            pdb_snapshot=pdb_snapshot,
            n=n,
            local_folder=TMP_FOLDER,
            load_live=load_live,
            sabdab=sabdab,
            sabdab_data_path=sabdab_data_path,
            require_antigen=require_antigen,
            max_chains=max_chains,
            pdb_id_list_path=pdb_id_list_path,
        )
        for id in error_ids:
            with open(LOG_FILE, &#34;a&#34;) as f:
                f.write(f&#34;&lt;&lt;&lt; Could not download PDB/mmCIF file: {id} \n&#34;)
        # paths = [(&#34;data/2c2m-1.pdb.gz&#34;, &#34;data/2c2m.fasta&#34;)]
        print(&#34;Filter and process...&#34;)
        _ = p_map(
            lambda x: process_f(x, force=force, sabdab=sabdab, ligand=load_ligands),
            paths,
        )
        # _ = [
        #     process_f(x, force=force, sabdab=sabdab, show_error=True)
        #     for x in tqdm(paths)
        # ]
    except Exception as e:
        _raise_rcsbsearch(e)

    stats = get_error_summary(LOG_FILE, verbose=False)
    not_found_error = &#34;&lt;&lt;&lt; PDB / mmCIF file downloaded but not found&#34;
    if not sabdab:
        while not_found_error in stats:
            with open(LOG_FILE) as f:
                lines = [x for x in f.readlines() if not x.startswith(not_found_error)]
            os.remove(LOG_FILE)
            with open(f&#34;{LOG_FILE}_tmp&#34;, &#34;a&#34;) as f:
                for line in lines:
                    f.write(line)
            if sabdab:
                paths = [
                    (
                        os.path.join(TMP_FOLDER, x.split(&#34;-&#34;)[0] + &#34;.pdb&#34;),
                        x.split(&#34;-&#34;)[1],
                    )
                    for x in stats[not_found_error]
                ]
            else:
                paths = stats[not_found_error]
            _ = p_map(lambda x: process_f(x, force=force, sabdab=sabdab), paths)
            stats = get_error_summary(LOG_FILE, verbose=False)
    if os.path.exists(f&#34;{LOG_FILE}_tmp&#34;):
        with open(LOG_FILE) as f:
            lines = [x for x in f.readlines() if not x.startswith(not_found_error)]
        os.remove(LOG_FILE)
        with open(f&#34;{LOG_FILE}_tmp&#34;, &#34;a&#34;) as f:
            for line in lines:
                f.write(line)
        os.rename(f&#34;{LOG_FILE}_tmp&#34;, LOG_FILE)

    if remove_redundancies:
        removed = _remove_database_redundancies(
            OUTPUT_FOLDER,
            seq_identity_threshold=redundancy_thr,
            ligand_identity=load_ligands,
        )
        _log_removed(removed, LOG_FILE)

    return get_error_summary(LOG_FILE)


def filter_and_convert(
    pdb_entry,
    min_length=50,
    max_length=150,
    max_missing_ends=5,
    max_missing_middle=5,
    load_ligands: bool = False,
    require_ligand: bool = False,
):
    &#34;&#34;&#34;Filter and convert a PDBEntry to a ProteinEntry.

    Parameters
    ----------
    pdb_entry : PDBEntry
        PDBEntry to be converted
    min_length : int, default 50
        Minimum total length of the protein sequence
    max_length : int, default 150
        Maximum total length of the protein sequence
    missing_ends_thr : float, default 0.3
        The maximum fraction of missing residues at the ends
    missing_middle_thr : float, default 0.1
        The maximum fraction of missing residues in the middle (after missing ends are disregarded)
    load_ligands: boool, default False
        Whether or not to load the ligands in the pdbs
    require_ligand: bool, default False
        Whether or not to require the presence of ligands

    Returns
    -------
    ProteinEntry
        The converted ProteinEntry

    &#34;&#34;&#34;
    pdb_dict = {}
    fasta_dict = pdb_entry.get_fasta()
    loaded_ligands = False
    if load_ligands and pdb_entry.get_ligands() is not None:
        ligand_dict = pdb_entry.get_ligands()
        if len(ligand_dict) &gt; 0:
            loaded_ligands = True
    if require_ligand and not loaded_ligands:
        raise PDBError(&#34;No ligands found&#34;)

    if len(pdb_entry.get_chains()) == 0:
        raise PDBError(&#34;No chains found&#34;)

    if pdb_entry.has_unnatural_amino_acids():
        raise PDBError(&#34;Unnatural amino acids found&#34;)

    for chain in pdb_entry.get_chains():
        pdb_dict[chain] = {}
        chain_crd = pdb_entry.get_sequence_df(chain)
        fasta_seq = fasta_dict[chain]

        if len(chain_crd) / len(fasta_seq) &lt; 1 - (
            max_missing_ends + max_missing_middle
        ):
            raise PDBError(&#34;Too many missing values in total&#34;)

        # align fasta and pdb and check criteria)
        mask = pdb_entry.get_mask([chain])[chain]
        known_ind = np.where(mask == 1)[0]
        start, end = known_ind[0], known_ind[-1] + 1
        if start + (len(mask) - end) &gt; max_missing_ends * len(mask):
            raise PDBError(&#34;Too many missing values in the ends&#34;)
        if (1 - mask)[start:end].sum() &gt; max_missing_middle * (end - start):
            raise PDBError(&#34;Too many missing values in the middle&#34;)
        if isinstance(pdb_entry, SAbDabEntry):
            pdb_dict[chain][&#34;cdr&#34;] = pdb_entry.get_cdr([chain])[chain]
        pdb_dict[chain][&#34;seq&#34;] = fasta_seq
        pdb_dict[chain][&#34;msk&#34;] = mask
        if min_length is not None and mask.sum() &lt; min_length:
            raise PDBError(&#34;Sequence is too short&#34;)
        if max_length is not None and len(mask) &gt; max_length:
            raise PDBError(&#34;Sequence is too long&#34;)

        # go over rows of coordinates
        crd_arr = pdb_entry.get_coordinates_array(chain)

        pdb_dict[chain][&#34;crd_bb&#34;] = crd_arr[:, :4, :]
        pdb_dict[chain][&#34;crd_sc&#34;] = crd_arr[:, 4:, :]
        pdb_dict[chain][&#34;msk&#34;][(pdb_dict[chain][&#34;crd_bb&#34;] == 0).sum(-1).sum(-1) &gt; 0] = 0
        if loaded_ligands:
            if chain in ligand_dict.keys():
                pdb_dict[chain][&#34;ligand&#34;] = ligand_dict[chain]
        if (pdb_dict[chain][&#34;msk&#34;][start:end] == 0).sum() &gt; max_missing_middle * (
            end - start
        ):
            raise PDBError(&#34;Too many missing values in the middle&#34;)
    return pdb_dict


def _remove_database_redundancies(
    dir, seq_identity_threshold=0.9, ligand_identity=False
):
    &#34;&#34;&#34;Remove all biounits in the database that are copies to another biounits in terms of sequence.

    Sequence identity is defined by the &#39;seq_identity_threshold&#39; parameter for robust detection of sequence similarity (missing residues, point mutations, ...).

    Parameters
    ----------
    dir : str
        the path to the database where all the biounits are stored in pickle files after their processing
    seq_identity_threshold : float, default .9
        the threshold that determines up to what percentage identity sequences are considered as the same

    Returns
    -------
    total_removed : int
        the total number of removed biounits

    &#34;&#34;&#34;
    all_files = np.array(os.listdir(dir))
    all_pdbs = np.array([file[:4] for file in all_files])
    pdb_counts = Counter(all_pdbs)
    pdbs_to_check = [pdb for pdb in pdb_counts.keys() if pdb_counts[pdb] &gt; 1]
    total_removed = []

    for pdb in tqdm(pdbs_to_check):
        biounits_list = np.array(
            [os.path.join(dir, file) for file in all_files[all_pdbs == pdb]]
        )
        biounits_list = sorted(biounits_list)
        redundancies = _check_biounits(
            biounits_list, seq_identity_threshold, ligand_identity
        )
        if redundancies != []:
            for k in redundancies:
                total_removed.append(os.path.basename(biounits_list[k]).split(&#34;.&#34;)[0])
                subprocess.run([&#34;rm&#34;, biounits_list[k]])

    return total_removed


def _open_pdb(file):
    &#34;&#34;&#34;Open a PDB file in the pickle format that follows the dwnloading and processing of the database.&#34;&#34;&#34;
    with open(file, &#34;rb&#34;) as f:
        return pickle.load(f)


def _check_biounits(biounits_list, threshold, ligand_identity):
    &#34;&#34;&#34;Return the indexes of the redundant biounits within the list of files given by `biounits_list`.&#34;&#34;&#34;
    biounits = [_open_pdb(b) for b in biounits_list]
    indexes = []

    for k, b1 in enumerate(biounits):
        if k not in indexes:
            b1_seqs = [b1[chain][&#34;seq&#34;] for chain in b1.keys()]
            for i, b2 in enumerate(biounits[k + 1 :]):
                if len(b1.keys()) != len(b2.keys()):
                    continue

                b2_seqs = [b2[chain][&#34;seq&#34;] for chain in b2.keys()]
                if ligand_identity:
                    ligs1 = []
                    for chain in b1.keys():
                        if &#34;ligand&#34; in b1[chain].keys():
                            ligs1.append(
                                &#34;.&#34;.join(
                                    list([c[&#34;smiles&#34;] for c in b1[chain][&#34;ligand&#34;]])
                                )
                            )
                    ligs2 = []
                    for chain in b2.keys():
                        if &#34;ligand&#34; in b2[chain].keys():
                            ligs2.append(
                                &#34;.&#34;.join(
                                    list([c[&#34;smiles&#34;] for c in b2[chain][&#34;ligand&#34;]])
                                )
                            )
                    equal_ligands = _compare_smiles(ligs1, ligs2, threshold)
                else:
                    equal_ligands = True
                if _compare_seqs(b1_seqs, b2_seqs, threshold) and equal_ligands:
                    indexes.append(k + i + 1)

    return indexes


def _compare_identity(seq, seqs, threshold):
    &#34;&#34;&#34;Assess whether a sequence is in a list of sequences (in the sense that it shares at least 90% to one of the sequences in the list).&#34;&#34;&#34;
    for s in seqs:
        if editdistance.eval(s, seq) / max(len(s), len(seq)) &lt;= (1 - threshold):
            return True

    return False


def _compare_seqs(seqs1, seqs2, threshold):
    &#34;&#34;&#34;Assess whether 2 lists of sequences contain exactly the same set of sequences.&#34;&#34;&#34;
    for seq in seqs1:
        if not _compare_identity(seq, seqs2, threshold):
            return False

    for seq in seqs2:
        if not _compare_identity(seq, seqs1, threshold):
            return False

    return True


def _raise_rcsbsearch(e):
    &#34;&#34;&#34;Raise a RuntimeError if the error is due to rcsbsearch.&#34;&#34;&#34;
    if &#34;404 Client Error&#34; in str(e):
        raise RuntimeError(
            &#39;Querying rcsbsearch is failing. Please install a version of rcsbsearch where this error is solved:\npython -m pip install &#34;rcsbsearch @ git+https://github.com/sbliven/rcsbsearch@dbdfe3880cc88b0ce57163987db613d579400c8e&#34;&#39;
        )
    else:
        raise e</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="proteinflow.processing.filter_and_convert"><code class="name flex">
<span>def <span class="ident">filter_and_convert</span></span>(<span>pdb_entry, min_length=50, max_length=150, max_missing_ends=5, max_missing_middle=5, load_ligands: bool = False, require_ligand: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Filter and convert a PDBEntry to a ProteinEntry.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>pdb_entry</code></strong> :&ensp;<code>PDBEntry</code></dt>
<dd>PDBEntry to be converted</dd>
<dt><strong><code>min_length</code></strong> :&ensp;<code>int</code>, default <code>50</code></dt>
<dd>Minimum total length of the protein sequence</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>int</code>, default <code>150</code></dt>
<dd>Maximum total length of the protein sequence</dd>
<dt><strong><code>missing_ends_thr</code></strong> :&ensp;<code>float</code>, default <code>0.3</code></dt>
<dd>The maximum fraction of missing residues at the ends</dd>
<dt><strong><code>missing_middle_thr</code></strong> :&ensp;<code>float</code>, default <code>0.1</code></dt>
<dd>The maximum fraction of missing residues in the middle (after missing ends are disregarded)</dd>
<dt><strong><code>load_ligands</code></strong> :&ensp;<code>boool</code>, default <code>False</code></dt>
<dd>Whether or not to load the ligands in the pdbs</dd>
<dt><strong><code>require_ligand</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>Whether or not to require the presence of ligands</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ProteinEntry</code></dt>
<dd>The converted ProteinEntry</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_and_convert(
    pdb_entry,
    min_length=50,
    max_length=150,
    max_missing_ends=5,
    max_missing_middle=5,
    load_ligands: bool = False,
    require_ligand: bool = False,
):
    &#34;&#34;&#34;Filter and convert a PDBEntry to a ProteinEntry.

    Parameters
    ----------
    pdb_entry : PDBEntry
        PDBEntry to be converted
    min_length : int, default 50
        Minimum total length of the protein sequence
    max_length : int, default 150
        Maximum total length of the protein sequence
    missing_ends_thr : float, default 0.3
        The maximum fraction of missing residues at the ends
    missing_middle_thr : float, default 0.1
        The maximum fraction of missing residues in the middle (after missing ends are disregarded)
    load_ligands: boool, default False
        Whether or not to load the ligands in the pdbs
    require_ligand: bool, default False
        Whether or not to require the presence of ligands

    Returns
    -------
    ProteinEntry
        The converted ProteinEntry

    &#34;&#34;&#34;
    pdb_dict = {}
    fasta_dict = pdb_entry.get_fasta()
    loaded_ligands = False
    if load_ligands and pdb_entry.get_ligands() is not None:
        ligand_dict = pdb_entry.get_ligands()
        if len(ligand_dict) &gt; 0:
            loaded_ligands = True
    if require_ligand and not loaded_ligands:
        raise PDBError(&#34;No ligands found&#34;)

    if len(pdb_entry.get_chains()) == 0:
        raise PDBError(&#34;No chains found&#34;)

    if pdb_entry.has_unnatural_amino_acids():
        raise PDBError(&#34;Unnatural amino acids found&#34;)

    for chain in pdb_entry.get_chains():
        pdb_dict[chain] = {}
        chain_crd = pdb_entry.get_sequence_df(chain)
        fasta_seq = fasta_dict[chain]

        if len(chain_crd) / len(fasta_seq) &lt; 1 - (
            max_missing_ends + max_missing_middle
        ):
            raise PDBError(&#34;Too many missing values in total&#34;)

        # align fasta and pdb and check criteria)
        mask = pdb_entry.get_mask([chain])[chain]
        known_ind = np.where(mask == 1)[0]
        start, end = known_ind[0], known_ind[-1] + 1
        if start + (len(mask) - end) &gt; max_missing_ends * len(mask):
            raise PDBError(&#34;Too many missing values in the ends&#34;)
        if (1 - mask)[start:end].sum() &gt; max_missing_middle * (end - start):
            raise PDBError(&#34;Too many missing values in the middle&#34;)
        if isinstance(pdb_entry, SAbDabEntry):
            pdb_dict[chain][&#34;cdr&#34;] = pdb_entry.get_cdr([chain])[chain]
        pdb_dict[chain][&#34;seq&#34;] = fasta_seq
        pdb_dict[chain][&#34;msk&#34;] = mask
        if min_length is not None and mask.sum() &lt; min_length:
            raise PDBError(&#34;Sequence is too short&#34;)
        if max_length is not None and len(mask) &gt; max_length:
            raise PDBError(&#34;Sequence is too long&#34;)

        # go over rows of coordinates
        crd_arr = pdb_entry.get_coordinates_array(chain)

        pdb_dict[chain][&#34;crd_bb&#34;] = crd_arr[:, :4, :]
        pdb_dict[chain][&#34;crd_sc&#34;] = crd_arr[:, 4:, :]
        pdb_dict[chain][&#34;msk&#34;][(pdb_dict[chain][&#34;crd_bb&#34;] == 0).sum(-1).sum(-1) &gt; 0] = 0
        if loaded_ligands:
            if chain in ligand_dict.keys():
                pdb_dict[chain][&#34;ligand&#34;] = ligand_dict[chain]
        if (pdb_dict[chain][&#34;msk&#34;][start:end] == 0).sum() &gt; max_missing_middle * (
            end - start
        ):
            raise PDBError(&#34;Too many missing values in the middle&#34;)
    return pdb_dict</code></pre>
</details>
</dd>
<dt id="proteinflow.processing.run_processing"><code class="name flex">
<span>def <span class="ident">run_processing</span></span>(<span>tmp_folder='./data/tmp_pdb', output_folder='./data/pdb', min_length=30, max_length=10000, resolution_thr=3.5, missing_ends_thr=0.3, missing_middle_thr=0.1, filter_methods=True, remove_redundancies=False, redundancy_thr=0.9, n=None, force=False, tag=None, pdb_snapshot=None, load_live=False, sabdab=False, sabdab_data_path=None, require_antigen=False, max_chains=5, pdb_id_list_path=None, load_ligands=False, require_ligand=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Download and parse PDB files that meet filtering criteria.</p>
<p>The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are
the following:</p>
<ul>
<li><code>'crd_bb'</code>: a <code>numpy</code> array of shape <code>(L, 4, 3)</code> with backbone atom coordinates (N, C, CA, O),</li>
<li><code>'crd_sc'</code>: a <code>numpy</code> array of shape <code>(L, 10, 3)</code> with sidechain atom coordinates (in a fixed order, check <code>sidechain_order()</code>),</li>
<li><code>'msk'</code>: a <code>numpy</code> array of shape <code>(L,)</code> where ones correspond to residues with known coordinates and
zeros to missing values,</li>
<li><code>'seq'</code>: a string of length <code>L</code> with residue types.</li>
</ul>
<p>When creating a SAbDab dataset, an additional key is added to the dictionary:
- <code>'cdr'</code>: a <code>'numpy'</code> array of shape <code>(L,)</code> where CDR residues are marked with the corresponding type (<code>'H1'</code>, <code>'L1'</code>, &hellip;)
and non-CDR residues are marked with <code>'-'</code>.</p>
<p>All errors including reasons for filtering a file out are logged in a log file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tmp_folder</code></strong> :&ensp;<code>str</code>, default <code>"./data/tmp_pdb"</code></dt>
<dd>The folder where temporary files will be saved</dd>
<dt><strong><code>output_folder</code></strong> :&ensp;<code>str</code>, default <code>"./data/pdb"</code></dt>
<dd>The folder where the output files will be saved</dd>
<dt><strong><code>min_length</code></strong> :&ensp;<code>int</code>, default <code>30</code></dt>
<dd>The minimum number of non-missing residues per chain</dd>
<dt><strong><code>max_length</code></strong> :&ensp;<code>int</code>, default <code>10000</code></dt>
<dd>The maximum number of residues per chain (set None for no threshold)</dd>
<dt><strong><code>resolution_thr</code></strong> :&ensp;<code>float</code>, default <code>3.5</code></dt>
<dd>The maximum resolution</dd>
<dt><strong><code>missing_ends_thr</code></strong> :&ensp;<code>float</code>, default <code>0.3</code></dt>
<dd>The maximum fraction of missing residues at the ends</dd>
<dt><strong><code>missing_middle_thr</code></strong> :&ensp;<code>float</code>, default <code>0.1</code></dt>
<dd>The maximum fraction of missing residues in the middle (after missing ends are disregarded)</dd>
<dt><strong><code>filter_methods</code></strong> :&ensp;<code>bool</code>, default <code>True</code></dt>
<dd>If <code>True</code>, only files obtained with X-ray or EM will be processed</dd>
<dt><strong><code>remove_redundancies</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>If <code>True</code>, removes biounits that are doubles of others sequence wise</dd>
<dt><strong><code>redundancy_thr</code></strong> :&ensp;<code>float</code>, default <code>0.9</code></dt>
<dd>The threshold upon which sequences are considered as one and the same (default: 90%)</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, default <code>None</code></dt>
<dd>The number of files to process (for debugging purposes)</dd>
<dt><strong><code>force</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>When <code>True</code>, rewrite the files if they already exist</dd>
<dt><strong><code>tag</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>A tag to add to the log file</dd>
<dt><strong><code>pdb_snapshot</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the PDB snapshot to use, by default the latest is used (if <code>sabdab</code> is <code>True</code>, you can use any date in the format YYYYMMDD as a cutoff)</dd>
<dt><strong><code>load_live</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, load the files that are not in the latest PDB snapshot from the PDB FTP server (forced to <code>False</code> if <code>pdb_snapshot</code> is not <code>None</code>)</dd>
<dt><strong><code>sabdab</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, download the SAbDab database instead of PDB</dd>
<dt><strong><code>sabdab_data_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>path to a zip file or a directory containing SAbDab files (only used if <code>sabdab</code> is <code>True</code>)</dd>
<dt><strong><code>require_antigen</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, only keep files with antigen chains (only used if <code>sabdab</code> is <code>True</code>)</dd>
<dt><strong><code>max_chains</code></strong> :&ensp;<code>int</code>, default <code>5</code></dt>
<dd>the maximum number of chains per biounit</dd>
<dt><strong><code>pdb_id_list_path</code></strong> :&ensp;<code>str</code>, default <code>None</code></dt>
<dd>if provided, get pdb_ids from list (format pdb_id-num example: 1XYZ-1)</dd>
<dt><strong><code>load_ligands</code></strong> :&ensp;<code>boool</code>, default <code>False</code></dt>
<dd>Whether or not to load the ligands in the pdbs</dd>
<dt><strong><code>require_ligand</code></strong> :&ensp;<code>bool</code>, default <code>False</code></dt>
<dd>if <code>True</code>, only keep files with ligands</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>log</code></strong> :&ensp;<code>dict</code></dt>
<dd>a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_processing(
    tmp_folder=&#34;./data/tmp_pdb&#34;,
    output_folder=&#34;./data/pdb&#34;,
    min_length=30,
    max_length=10000,
    resolution_thr=3.5,
    missing_ends_thr=0.3,
    missing_middle_thr=0.1,
    filter_methods=True,
    remove_redundancies=False,
    redundancy_thr=0.9,
    n=None,
    force=False,
    tag=None,
    pdb_snapshot=None,
    load_live=False,
    sabdab=False,
    sabdab_data_path=None,
    require_antigen=False,
    max_chains=5,
    pdb_id_list_path=None,
    load_ligands=False,
    require_ligand=False,
):
    &#34;&#34;&#34;Download and parse PDB files that meet filtering criteria.

    The output files are pickled nested dictionaries where first-level keys are chain Ids and second-level keys are
    the following:

    - `&#39;crd_bb&#39;`: a `numpy` array of shape `(L, 4, 3)` with backbone atom coordinates (N, C, CA, O),
    - `&#39;crd_sc&#39;`: a `numpy` array of shape `(L, 10, 3)` with sidechain atom coordinates (in a fixed order, check `sidechain_order()`),
    - `&#39;msk&#39;`: a `numpy` array of shape `(L,)` where ones correspond to residues with known coordinates and
        zeros to missing values,
    - `&#39;seq&#39;`: a string of length `L` with residue types.

    When creating a SAbDab dataset, an additional key is added to the dictionary:
    - `&#39;cdr&#39;`: a `&#39;numpy&#39;` array of shape `(L,)` where CDR residues are marked with the corresponding type (`&#39;H1&#39;`, `&#39;L1&#39;`, ...)
        and non-CDR residues are marked with `&#39;-&#39;`.

    All errors including reasons for filtering a file out are logged in a log file.

    Parameters
    ----------
    tmp_folder : str, default &#34;./data/tmp_pdb&#34;
        The folder where temporary files will be saved
    output_folder : str, default &#34;./data/pdb&#34;
        The folder where the output files will be saved
    min_length : int, default 30
        The minimum number of non-missing residues per chain
    max_length : int, default 10000
        The maximum number of residues per chain (set None for no threshold)
    resolution_thr : float, default 3.5
        The maximum resolution
    missing_ends_thr : float, default 0.3
        The maximum fraction of missing residues at the ends
    missing_middle_thr : float, default 0.1
        The maximum fraction of missing residues in the middle (after missing ends are disregarded)
    filter_methods : bool, default True
        If `True`, only files obtained with X-ray or EM will be processed
    remove_redundancies : bool, default False
        If `True`, removes biounits that are doubles of others sequence wise
    redundancy_thr : float, default 0.9
        The threshold upon which sequences are considered as one and the same (default: 90%)
    n : int, default None
        The number of files to process (for debugging purposes)
    force : bool, default False
        When `True`, rewrite the files if they already exist
    tag : str, optional
        A tag to add to the log file
    pdb_snapshot : str, optional
        the PDB snapshot to use, by default the latest is used (if `sabdab` is `True`, you can use any date in the format YYYYMMDD as a cutoff)
    load_live : bool, default False
        if `True`, load the files that are not in the latest PDB snapshot from the PDB FTP server (forced to `False` if `pdb_snapshot` is not `None`)
    sabdab : bool, default False
        if `True`, download the SAbDab database instead of PDB
    sabdab_data_path : str, optional
        path to a zip file or a directory containing SAbDab files (only used if `sabdab` is `True`)
    require_antigen : bool, default False
        if `True`, only keep files with antigen chains (only used if `sabdab` is `True`)
    max_chains : int, default 5
        the maximum number of chains per biounit
    pdb_id_list_path : str, default None
        if provided, get pdb_ids from list (format pdb_id-num example: 1XYZ-1)
    load_ligands: boool, default False
        Whether or not to load the ligands in the pdbs
    require_ligand: bool, default False
        if `True`, only keep files with ligands

    Returns
    -------
    log : dict
        a dictionary where keys are recognized error names and values are lists of PDB ids that caused the errors

    &#34;&#34;&#34;
    TMP_FOLDER = tmp_folder
    OUTPUT_FOLDER = output_folder
    MIN_LENGTH = min_length
    MAX_LENGTH = max_length
    RESOLUTION_THR = resolution_thr
    MISSING_ENDS_THR = missing_ends_thr
    MISSING_MIDDLE_THR = missing_middle_thr

    if not os.path.exists(TMP_FOLDER):
        os.makedirs(TMP_FOLDER)
    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)

    LOG_FILE = os.path.join(OUTPUT_FOLDER, &#34;log.txt&#34;)
    print(f&#34;Log file: {LOG_FILE} \n&#34;)
    now = datetime.now()  # current date and time
    date_time = now.strftime(&#34;%m/%d/%Y, %H:%M:%S&#34;) + &#34;\n\n&#34;
    with open(LOG_FILE, &#34;a&#34;) as f:
        f.write(date_time)
        if tag is not None:
            f.write(f&#34;tag: {tag} \n&#34;)
        f.write(f&#34;    min_length: {min_length} \n&#34;)
        f.write(f&#34;    max_length: {max_length} \n&#34;)
        f.write(f&#34;    resolution_thr: {resolution_thr} \n&#34;)
        f.write(f&#34;    missing_ends_thr: {missing_ends_thr} \n&#34;)
        f.write(f&#34;    missing_middle_thr: {missing_middle_thr} \n&#34;)
        f.write(f&#34;    filter_methods: {filter_methods} \n&#34;)
        f.write(f&#34;    remove_redundancies: {remove_redundancies} \n&#34;)
        f.write(f&#34;    sabdab: {sabdab} \n&#34;)
        f.write(f&#34;    pdb_snapshot: {pdb_snapshot} \n&#34;)
        f.write(f&#34;    max_chains: {max_chains} \n&#34;)
        if remove_redundancies:
            f.write(f&#34;    redundancy_threshold: {redundancy_thr} \n&#34;)
        if sabdab:
            f.write(f&#34;    require_antigen: {require_antigen} \n&#34;)
            f.write(f&#34;    sabdab_data_path: {sabdab_data_path} \n&#34;)
        else:
            f.write(f&#34;    load_live: {load_live} \n&#34;)
        f.write(&#34;\n&#34;)

    def process_f(
        local_paths,
        show_error=False,
        force=True,
        sabdab=False,
        ligand=False,
    ):
        pdb_path, fasta_path = local_paths
        chain_id = None
        if sabdab:
            pdb_path, chain_id = pdb_path
            heavy, light, antigen = chain_id.split(&#34;_&#34;)
            if heavy == &#34;nan&#34;:
                heavy = None
            if light == &#34;nan&#34;:
                light = None
            if antigen == &#34;nan&#34;:
                antigen = []
            else:
                antigen = antigen.split(&#34; | &#34;)
        fn = os.path.basename(pdb_path)
        pdb_id = fn.split(&#34;.&#34;)[0]
        if os.path.getsize(pdb_path) &gt; 1e7:
            _log_exception(
                PDBError(&#34;PDB / mmCIF file is too large&#34;),
                LOG_FILE,
                pdb_id,
                TMP_FOLDER,
                chain_id=chain_id,
            )
        try:
            # local_path = download_f(pdb_id, s3_client=s3_client, load_live=load_live)
            name = pdb_id if not sabdab else pdb_id + &#34;-&#34; + chain_id
            target_file = os.path.join(OUTPUT_FOLDER, name + &#34;.pickle&#34;)
            if not force and os.path.exists(target_file):
                raise PDBError(&#34;File already exists&#34;)
            if sabdab:
                pdb_entry = SAbDabEntry(
                    pdb_path=pdb_path,
                    heavy_chain=heavy,
                    light_chain=light,
                    antigen_chains=antigen,
                    fasta_path=fasta_path,
                )
            else:
                pdb_entry = PDBEntry(
                    pdb_path=pdb_path, fasta_path=fasta_path, load_ligand=ligand
                )
            # filter and convert
            protein_dict = filter_and_convert(
                pdb_entry,
                min_length=MIN_LENGTH,
                max_length=MAX_LENGTH,
                max_missing_ends=MISSING_ENDS_THR,
                max_missing_middle=MISSING_MIDDLE_THR,
                load_ligands=ligand,
                require_ligand=require_ligand,
            )
            # save
            with open(target_file, &#34;wb&#34;) as f:
                pickle.dump(protein_dict, f)
        except Exception as e:
            if show_error:
                raise e
            else:
                _log_exception(e, LOG_FILE, pdb_id, TMP_FOLDER, chain_id=chain_id)

    try:
        paths, error_ids = _load_files(
            resolution_thr=RESOLUTION_THR,
            filter_methods=filter_methods,
            pdb_snapshot=pdb_snapshot,
            n=n,
            local_folder=TMP_FOLDER,
            load_live=load_live,
            sabdab=sabdab,
            sabdab_data_path=sabdab_data_path,
            require_antigen=require_antigen,
            max_chains=max_chains,
            pdb_id_list_path=pdb_id_list_path,
        )
        for id in error_ids:
            with open(LOG_FILE, &#34;a&#34;) as f:
                f.write(f&#34;&lt;&lt;&lt; Could not download PDB/mmCIF file: {id} \n&#34;)
        # paths = [(&#34;data/2c2m-1.pdb.gz&#34;, &#34;data/2c2m.fasta&#34;)]
        print(&#34;Filter and process...&#34;)
        _ = p_map(
            lambda x: process_f(x, force=force, sabdab=sabdab, ligand=load_ligands),
            paths,
        )
        # _ = [
        #     process_f(x, force=force, sabdab=sabdab, show_error=True)
        #     for x in tqdm(paths)
        # ]
    except Exception as e:
        _raise_rcsbsearch(e)

    stats = get_error_summary(LOG_FILE, verbose=False)
    not_found_error = &#34;&lt;&lt;&lt; PDB / mmCIF file downloaded but not found&#34;
    if not sabdab:
        while not_found_error in stats:
            with open(LOG_FILE) as f:
                lines = [x for x in f.readlines() if not x.startswith(not_found_error)]
            os.remove(LOG_FILE)
            with open(f&#34;{LOG_FILE}_tmp&#34;, &#34;a&#34;) as f:
                for line in lines:
                    f.write(line)
            if sabdab:
                paths = [
                    (
                        os.path.join(TMP_FOLDER, x.split(&#34;-&#34;)[0] + &#34;.pdb&#34;),
                        x.split(&#34;-&#34;)[1],
                    )
                    for x in stats[not_found_error]
                ]
            else:
                paths = stats[not_found_error]
            _ = p_map(lambda x: process_f(x, force=force, sabdab=sabdab), paths)
            stats = get_error_summary(LOG_FILE, verbose=False)
    if os.path.exists(f&#34;{LOG_FILE}_tmp&#34;):
        with open(LOG_FILE) as f:
            lines = [x for x in f.readlines() if not x.startswith(not_found_error)]
        os.remove(LOG_FILE)
        with open(f&#34;{LOG_FILE}_tmp&#34;, &#34;a&#34;) as f:
            for line in lines:
                f.write(line)
        os.rename(f&#34;{LOG_FILE}_tmp&#34;, LOG_FILE)

    if remove_redundancies:
        removed = _remove_database_redundancies(
            OUTPUT_FOLDER,
            seq_identity_threshold=redundancy_thr,
            ligand_identity=load_ligands,
        )
        _log_removed(removed, LOG_FILE)

    return get_error_summary(LOG_FILE)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="pdoc Home" href="https://adaptyvbio.github.io/ProteinFlow/">
<img src="https://raw.githubusercontent.com/adaptyvbio/ProteinFlow/main/media/logo.png" alt="">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="proteinflow" href="../index.html">proteinflow</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="proteinflow.processing.filter_and_convert" href="#proteinflow.processing.filter_and_convert">filter_and_convert</a></code></li>
<li><code><a title="proteinflow.processing.run_processing" href="#proteinflow.processing.run_processing">run_processing</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>