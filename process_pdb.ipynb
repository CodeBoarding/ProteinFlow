{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.parse_pdb import align_pdb, open_pdb, PDBError, get_pdb_file\n",
    "import os\n",
    "import boto3\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from p_tqdm import p_map\n",
    "import sidechainnet as scn\n",
    "import numpy as np\n",
    "from rcsbsearch import TextQuery, Attr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(id):\n",
    "    with open(f\"./data/pdb/{id}.pickle\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    crds = []\n",
    "    seq = \"\"\n",
    "    for chain in data:  \n",
    "        crd = np.concatenate([data[chain][\"crd_bb\"], data[chain][\"crd_sc\"]], axis=1).reshape((-1, 3))\n",
    "        crds.append(crd)\n",
    "        seq += data[chain][\"seq\"]\n",
    "    crd = np.concatenate(crds, 0)\n",
    "    sb2 = scn.StructureBuilder(seq, crd)\n",
    "    return sb2.to_3Dmol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_log_stats(log_file):\n",
    "    stats = defaultdict(lambda: 0)\n",
    "    with open(log_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if line.startswith(\"<<<\"):\n",
    "                stats[line.split(':')[0]] += 1\n",
    "    keys = sorted(stats.keys(), key=lambda x: stats[x], reverse=True)\n",
    "    for key in keys:\n",
    "        value = stats[key]\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unknown_stats(log_file):\n",
    "    stats = defaultdict(lambda: [])\n",
    "    with open(log_file, \"r\") as f:\n",
    "        error = None\n",
    "        id = None\n",
    "        for line in f.readlines():\n",
    "            if line.startswith(\"<<< Unknown\"):\n",
    "                error = \"\"\n",
    "                id = line.split(\":\")[-1].strip()\n",
    "            elif line.startswith(\"<<<\") and error is not None:\n",
    "                if error.startswith(\"Could not download\"):\n",
    "                    error = \"Could not download PDB\"\n",
    "                stats[error].append(id)\n",
    "                error = None\n",
    "            elif error is not None:\n",
    "                error += line\n",
    "    keys = sorted(stats.keys(), key=lambda x: len(stats[x]), reverse=True)\n",
    "    for key in keys:\n",
    "        value = stats[key]\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.parse_pdb import get_pdb_file\n",
    "bucket = boto3.resource('s3').Bucket(\"pdbsnapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('pdbsnapshots')\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "from collections import namedtuple\n",
    "from operator import attrgetter\n",
    "\n",
    "\n",
    "S3Obj = namedtuple('S3Obj', ['key', 'mtime', 'size', 'ETag'])\n",
    "\n",
    "\n",
    "def s3list(bucket, path, start=None, end=None, recursive=True, list_dirs=True,\n",
    "           list_objs=True, limit=None):\n",
    "    \"\"\"\n",
    "    Iterator that lists a bucket's objects under path, (optionally) starting with\n",
    "    start and ending before end.\n",
    "\n",
    "    If recursive is False, then list only the \"depth=0\" items (dirs and objects).\n",
    "\n",
    "    If recursive is True, then list recursively all objects (no dirs).\n",
    "\n",
    "    Args:\n",
    "        bucket:\n",
    "            a boto3.resource('s3').Bucket().\n",
    "        path:\n",
    "            a directory in the bucket.\n",
    "        start:\n",
    "            optional: start key, inclusive (may be a relative path under path, or\n",
    "            absolute in the bucket)\n",
    "        end:\n",
    "            optional: stop key, exclusive (may be a relative path under path, or\n",
    "            absolute in the bucket)\n",
    "        recursive:\n",
    "            optional, default True. If True, lists only objects. If False, lists\n",
    "            only depth 0 \"directories\" and objects.\n",
    "        list_dirs:\n",
    "            optional, default True. Has no effect in recursive listing. On\n",
    "            non-recursive listing, if False, then directories are omitted.\n",
    "        list_objs:\n",
    "            optional, default True. If False, then directories are omitted.\n",
    "        limit:\n",
    "            optional. If specified, then lists at most this many items.\n",
    "\n",
    "    Returns:\n",
    "        an iterator of S3Obj.\n",
    "\n",
    "    Examples:\n",
    "        # set up\n",
    "        >>> s3 = boto3.resource('s3')\n",
    "        ... bucket = s3.Bucket('bucket-name')\n",
    "\n",
    "        # iterate through all S3 objects under some dir\n",
    "        >>> for p in s3list(bucket, 'some/dir'):\n",
    "        ...     print(p)\n",
    "\n",
    "        # iterate through up to 20 S3 objects under some dir, starting with foo_0010\n",
    "        >>> for p in s3list(bucket, 'some/dir', limit=20, start='foo_0010'):\n",
    "        ...     print(p)\n",
    "\n",
    "        # non-recursive listing under some dir:\n",
    "        >>> for p in s3list(bucket, 'some/dir', recursive=False):\n",
    "        ...     print(p)\n",
    "\n",
    "        # non-recursive listing under some dir, listing only dirs:\n",
    "        >>> for p in s3list(bucket, 'some/dir', recursive=False, list_objs=False):\n",
    "        ...     print(p)\n",
    "\"\"\"\n",
    "    kwargs = dict()\n",
    "    if start is not None:\n",
    "        if not start.startswith(path):\n",
    "            start = os.path.join(path, start)\n",
    "        # note: need to use a string just smaller than start, because\n",
    "        # the list_object API specifies that start is excluded (the first\n",
    "        # result is *after* start).\n",
    "        kwargs.update(Marker=__prev_str(start))\n",
    "    if end is not None:\n",
    "        if not end.startswith(path):\n",
    "            end = os.path.join(path, end)\n",
    "    if not recursive:\n",
    "        kwargs.update(Delimiter='/')\n",
    "        if not path.endswith('/') and len(path) > 0:\n",
    "            path += '/'\n",
    "    kwargs.update(Prefix=path)\n",
    "    if limit is not None:\n",
    "        kwargs.update(PaginationConfig={'MaxItems': limit})\n",
    "\n",
    "    paginator = bucket.meta.client.get_paginator('list_objects')\n",
    "    for resp in paginator.paginate(Bucket=bucket.name, **kwargs):\n",
    "        q = []\n",
    "        if 'CommonPrefixes' in resp and list_dirs:\n",
    "            q = [S3Obj(f['Prefix'], None, None, None) for f in resp['CommonPrefixes']]\n",
    "        if 'Contents' in resp and list_objs:\n",
    "            q += [S3Obj(f['Key'], f['LastModified'], f['Size'], f['ETag']) for f in resp['Contents']]\n",
    "        # note: even with sorted lists, it is faster to sort(a+b)\n",
    "        # than heapq.merge(a, b) at least up to 10K elements in each list\n",
    "        q = sorted(q, key=attrgetter('key'))\n",
    "        if limit is not None:\n",
    "            q = q[:limit]\n",
    "            limit -= len(q)\n",
    "        for p in q:\n",
    "            if end is not None and p.key >= end:\n",
    "                return\n",
    "            yield p\n",
    "\n",
    "folders = [x.key for x in s3list(bucket, \"\", recursive=False, list_objs=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file='20220103/pub/pdb/data/biounit/PDB/all/2btj.pdb1.gz'\n",
      "file='20210105/pub/pdb/data/biounit/PDB/all/2btj.pdb1.gz'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/tmp_pdb/2btj-1.pdb.gz'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDB_PREFIX = \"pub/pdb/data/biounit/PDB/all/\"\n",
    "ordered_folders = [x.key + PDB_PREFIX for x in s3list(boto3.resource('s3').Bucket(\"pdbsnapshots\"), \"\", recursive=False, list_objs=False)]\n",
    "ordered_folders = sorted(ordered_folders, reverse=True)\n",
    "get_pdb_file(\"2btj.pdb1.gz\", bucket, \"data/tmp_pdb\", folders=ordered_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.download_file(\"20210105//pub/pdb/data/biounit/PDB/all/2btj.pdb1.gz\", \"data/tmp_pdb/file.pdb.gz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
