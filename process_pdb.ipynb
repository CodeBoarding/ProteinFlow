{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.parse_pdb import align_pdb, open_pdb, PDBError, get_pdb_file\n",
    "import os\n",
    "import boto3\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from p_tqdm import p_map\n",
    "import sidechainnet as scn\n",
    "import numpy as np\n",
    "from rcsbsearch import TextQuery, Attr\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(id):\n",
    "    with open(f\"./data/pdb/{id}.pickle\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    crds = []\n",
    "    seq = \"\"\n",
    "    for chain in data:  \n",
    "        crd = np.concatenate([data[chain][\"crd_bb\"], data[chain][\"crd_sc\"]], axis=1).reshape((-1, 3))\n",
    "        crds.append(crd)\n",
    "        seq += data[chain][\"seq\"]\n",
    "    crd = np.concatenate(crds, 0)\n",
    "    sb2 = scn.StructureBuilder(seq, crd)\n",
    "    return sb2.to_3Dmol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_log_stats(log_file):\n",
    "    stats = defaultdict(lambda: 0)\n",
    "    with open(log_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if line.startswith(\"<<<\"):\n",
    "                stats[line.split(':')[0]] += 1\n",
    "    keys = sorted(stats.keys(), key=lambda x: stats[x], reverse=True)\n",
    "    for key in keys:\n",
    "        value = stats[key]\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unknown_stats(log_file):\n",
    "    stats = defaultdict(lambda: [])\n",
    "    with open(log_file, \"r\") as f:\n",
    "        error = None\n",
    "        id = None\n",
    "        for line in f.readlines():\n",
    "            if line.startswith(\"<<< Unknown\"):\n",
    "                error = \"\"\n",
    "                id = line.split(\":\")[-1].strip()\n",
    "            elif line.startswith(\"<<<\") and error is not None:\n",
    "                if error.startswith(\"Could not download\"):\n",
    "                    error = \"Could not download PDB\"\n",
    "                stats[error].append(id)\n",
    "                error = None\n",
    "            elif error is not None:\n",
    "                error += line\n",
    "    keys = sorted(stats.keys(), key=lambda x: len(stats[x]), reverse=True)\n",
    "    for key in keys:\n",
    "        value = stats[key]\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.parse_pdb import get_pdb_file\n",
    "bucket = boto3.resource('s3').Bucket(\"pdbsnapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to load BestProt data\n",
    "\n",
    "    Saves the model input tensors as pickle files in `features_folder`. When `clustering_dict_path` is provided,\n",
    "    at each iteration a random bionit from a cluster is sampled.\n",
    "\n",
    "    Returns dictionaries with the following keys and values (all values are `torch` tensors):\n",
    "    - `'X'`: 3D coordinates of N, C, Ca, O (shape `(total_L, 4, 3)`),\n",
    "    - `'S'`: sequence indices (shape `(total_L)`),\n",
    "    - `'mask'`: residue mask (0 where coordinates are missing, 1 otherwise) (shape `(total_L)`),\n",
    "    - `'residue_idx'`: residue indices (from 0 to length of sequence, +100 where chains change) (shape `(total_L)`),\n",
    "    - `'chain_encoding_all'`: chain indices (shape `(total_L)`),\n",
    "    - `'chain_id`': the chain id to mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            dataset_folder, \n",
    "            features_folder, \n",
    "            clustering_dict_path=None, \n",
    "            max_length=100, \n",
    "            rewrite=False,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_folder : str\n",
    "            the path to the folder with BestProt format input files (assumes that files are named {biounit_id}.pickle)\n",
    "        features_folder : str\n",
    "            the path to the folder where the ProteinMPNN features will be saved\n",
    "        clustering_dict_path : str, optional\n",
    "            path to the pickled clustering dictionary (keys are cluster ids, values are (biounit id, chain id) tuples)\n",
    "        max_length : int, default 100\n",
    "            entries with total length of chains larger than `max_length` will be disregarded\n",
    "        \"\"\"\n",
    "\n",
    "        alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "        self.alphabet_dict = {letter: i for i, letter in enumerate(alphabet)}\n",
    "        self.num_chains = {} # number of chains by biounit id\n",
    "        self.files = {} # file path by biounit id\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.features_folder = features_folder\n",
    "        output_tuples = p_map(lambda x: self._process(x, rewrite=rewrite), os.listdir(dataset_folder))\n",
    "        # output_tuples = tqdm([self._process(x, rewrite=rewrite) for x in os.listdir(dataset_folder)])\n",
    "        for id, filename, num_chains in output_tuples:\n",
    "            self.files[id] = filename\n",
    "            self.num_chains[id] = num_chains\n",
    "        for filename in os.listdir(dataset_folder):\n",
    "            self._process(filename)\n",
    "        if clustering_dict_path is not None:\n",
    "            with open(clustering_dict_path, \"rb\") as f:\n",
    "                self.clusters = pickle.load(f) # list of biounit ids by cluster id\n",
    "            self.data = list(self.clusters.keys())\n",
    "        else:\n",
    "            self.clusters = None\n",
    "            self.data = list(self.files.keys())\n",
    "        \n",
    "    \n",
    "    def _process(self, filename, rewrite=False):\n",
    "        \"\"\"\n",
    "        Process a BestProt file and save it as ProteinMPNN features\n",
    "        \"\"\"\n",
    "\n",
    "        input_file = os.path.join(self.dataset_folder, filename)\n",
    "        output_file = os.path.join(self.features_folder, filename)\n",
    "        with open(input_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        chains = sorted(data.keys())\n",
    "        if not rewrite and os.path.exists(output_file):\n",
    "            pass\n",
    "        else:\n",
    "            X = []\n",
    "            S = []\n",
    "            mask = []\n",
    "            chain_encoding_all = []\n",
    "            residue_idx = []\n",
    "            last_idx = 0\n",
    "            for chain_i, chain in enumerate(chains):\n",
    "                X.append(data[chain][\"crd_bb\"])\n",
    "                S += [self.alphabet_dict[x] for x in data[chain][\"seq\"]]\n",
    "                mask.append(data[chain][\"msk\"])\n",
    "                residue_idx.append(torch.arange(len(data[chain][\"seq\"])) + last_idx)\n",
    "                last_idx = residue_idx[-1][-1] + 100\n",
    "                chain_encoding_all.append(torch.ones(len(data[chain][\"seq\"])) * chain_i)\n",
    "            out = {}\n",
    "            out[\"X\"] = torch.from_numpy(np.concatenate(X, 0))\n",
    "            out[\"S\"] = torch.tensor(S)\n",
    "            out[\"mask\"] = torch.from_numpy(np.concatenate(mask))\n",
    "            out[\"chain_encoding_all\"] = torch.cat(chain_encoding_all)\n",
    "            out[\"residue_idx\"] = torch.cat(residue_idx)\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pickle.dump(out, f)\n",
    "        return os.path.basename(filename).split('.')[0], output_file, len(chains), \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chain_id = None\n",
    "        if self.clusters is None:\n",
    "            id = self.data[idx]\n",
    "            chain_id = random.randint(0, self.num_chains[id] - 1)\n",
    "        else:\n",
    "            cluster = self.data[idx]\n",
    "            id, chain_id = random.sample(self.clusters[cluster])\n",
    "        file = self.files[id]\n",
    "        with open(file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data[\"chain_id\"] = chain_id\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2131/2131 [00:00<00:00, 23187.14it/s]\n"
     ]
    }
   ],
   "source": [
    "folder = \"data/subset\"\n",
    "cnt = 0\n",
    "total = 0\n",
    "for file in tqdm(os.listdir(folder)):\n",
    "    with open(os.path.join(folder, file), \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    for chain in data:\n",
    "        total += 1\n",
    "        if \"-\" in data[chain][\"seq\"]:\n",
    "            os.remove(os.path.join(folder, file))\n",
    "            break\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2131/2131 [00:09<00:00, 217.91it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ProteinDataset(dataset_folder='data/subset', features_folder=\"data/tmp_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': tensor([[[18.1920, 25.9740, 29.7160],\n",
       "          [16.1630, 26.8260, 28.5070],\n",
       "          [16.7100, 26.1890, 29.7970],\n",
       "          [16.3770, 28.0150, 28.2540]],\n",
       " \n",
       "         [[15.4450, 26.0300, 27.7090],\n",
       "          [13.7730, 27.5300, 26.5110],\n",
       "          [14.8740, 26.4880, 26.4230],\n",
       "          [12.8350, 27.4050, 27.3130]],\n",
       " \n",
       "         [[13.8790, 28.5420, 25.6590],\n",
       "          [12.0540, 29.2960, 24.3770],\n",
       "          [12.8780, 29.5940, 25.6150],\n",
       "          [12.5720, 29.3250, 23.2550]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[21.4890, 17.9400, 29.5560],\n",
       "          [19.5650, 19.3690, 29.9680],\n",
       "          [20.4220, 18.6940, 28.8980],\n",
       "          [19.5650, 18.9460, 31.1240]],\n",
       " \n",
       "         [[18.8370, 20.4370, 29.6040],\n",
       "          [16.9570, 20.2990, 31.2810],\n",
       "          [17.9880, 21.1590, 30.5600],\n",
       "          [16.4870, 19.2980, 30.7420]],\n",
       " \n",
       "         [[16.6160, 20.7010, 32.5020],\n",
       "          [14.2280, 20.0150, 32.8040],\n",
       "          [15.6530, 19.9810, 33.3330],\n",
       "          [14.0100, 20.5590, 31.7000]]], dtype=torch.float64),\n",
       " 'S': tensor([12,  4,  3,  9, 15,  5,  8, 18,  7, 16, 15, 19,  7,  5, 15, 15,  2,  9,\n",
       "          3,  8,  7,  5,  3, 11,  0, 12,  4, 13, 17,  4, 10, 14, 15,  7,  3,  4,\n",
       "          2,  2,  8,  3, 15,  8, 17, 19,  9, 11,  4,  4, 15,  8,  3, 11,  5,  7,\n",
       "          1,  3,  3,  4, 15,  9,  7,  5, 16,  8, 13,  3,  5, 11, 16, 19,  2, 17,\n",
       "         11, 19,  0,  5, 11, 11,  8,  4, 17, 17, 15, 19,  0, 15,  3, 16,  0,  9,\n",
       "          7,  7, 15, 11,  7, 11, 17,  2,  3,  3,  5,  2,  8, 16,  7, 10, 16,  5,\n",
       "          9,  9,  5,  8,  5, 16,  2,  7,  3,  2, 13,  2,  9,  3,  8,  4,  8,  3,\n",
       "         17, 16, 14,  3, 11,  5,  7, 12,  3,  3, 11,  7, 17, 11,  7,  7,  3, 14,\n",
       "          2,  2,  1, 12,  0, 12,  4,  3,  9, 15,  5,  8, 18,  7, 16, 15, 19,  7,\n",
       "          5, 15, 15,  2,  9,  3,  8,  7,  5,  3, 11,  0, 12,  4, 13, 17,  4, 10,\n",
       "         14, 15,  7,  3,  4,  2,  2,  8,  3, 15,  8, 17, 19,  9, 11,  4,  4, 15,\n",
       "          8,  3, 11,  5,  7,  1,  3,  3,  4, 15,  9,  7,  5, 16,  8, 13,  3,  5,\n",
       "         11, 16, 19,  2, 17, 11, 19,  0,  5, 11, 11,  8,  4, 17, 17, 15, 19,  0,\n",
       "         15,  3, 16,  0,  9,  7,  7, 15, 11,  7, 11, 17,  2,  3,  3,  5,  2,  8,\n",
       "         16,  7, 10, 16,  5,  9,  9,  5,  8,  5, 16,  2,  7,  3,  2, 13,  2,  9,\n",
       "          3,  8,  4,  8,  3, 17, 16, 14,  3, 11,  5,  7, 12,  3,  3, 11,  7, 17,\n",
       "         11,  7,  7,  3, 14,  2,  2,  1, 12,  0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'chain_encoding_all': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'residue_idx': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "         140, 141, 142, 143, 144, 145, 146, 147, 148, 248, 249, 250, 251, 252,\n",
       "         253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
       "         267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
       "         281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
       "         295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "         309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
       "         323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
       "         337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "         351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
       "         365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
       "         379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
       "         393, 394, 395, 396]),\n",
       " 'chain_id': 0}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
